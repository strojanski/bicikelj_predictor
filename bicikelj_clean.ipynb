{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/bicikelj_train.csv\")\n",
    "train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n",
    "\n",
    "test = pd.read_csv(\"data/bicikelj_test.csv\")\n",
    "test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract new features\n",
    "def get_time_based_features(timestamp):\n",
    "    # From datetime64 get month, day, hour, dayOfWeek and isHoliday\n",
    "    timestamp = np.datetime64(timestamp)\n",
    "    dt_python = timestamp.astype(datetime.datetime)\n",
    "\n",
    "    # Extract month, day, hour, and day_of_week\n",
    "    month = dt_python.month\n",
    "    day = dt_python.day\n",
    "    hour = dt_python.hour\n",
    "    minute = dt_python.minute\n",
    "    day_of_week = dt_python.weekday()\n",
    "\n",
    "    # Check for holidays\n",
    "    is_holiday = 0\n",
    "    if dt_python.month == 8 and dt_python.day in [15, 17]:\n",
    "        is_holiday = 1\n",
    "    elif dt_python.month == 9 and dt_python.day in [15, 23]:\n",
    "        is_holiday = 1\n",
    "        \n",
    "        \n",
    "    is_weekend = 0\n",
    "    if dt_python.weekday() in [5, 6]:\n",
    "        is_weekend = 1\n",
    "   \n",
    "    is_night = 0\n",
    "    if hour < 6 or hour > 22:\n",
    "        is_night = 1\n",
    "   \n",
    "    return month, day, hour, minute, day_of_week, is_holiday, is_weekend, is_night\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of stations\n",
    "stations = []\n",
    "for i in range(1, 84):\n",
    "    stations.append(train.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_smaller_row(df, timestamp, hour_offset, thresh=15):\n",
    "    print(timestamp)\n",
    "    target_timestamp = pd.to_datetime(timestamp) - pd.DateOffset(hours=hour_offset)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    \n",
    "    smaller_rows = df[(pd.to_datetime(df[\"timestamp\"]) <= target_timestamp) & (df[\"timestamp\"] > target_timestamp - datetime.timedelta(minutes=thresh))]\n",
    "    if smaller_rows.empty:\n",
    "        # Exclude from training\n",
    "        return None\n",
    "    closest_index = np.argmin(np.abs(target_timestamp - pd.to_datetime(smaller_rows[\"timestamp\"])))\n",
    "    print(smaller_rows[\"timestamp\"], closest_index)\n",
    "    closest_row = smaller_rows.iloc[closest_index]\n",
    "    \n",
    "    return closest_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(df, timestamp, hr_offset=1, thresh=15):\n",
    "    target_ts = pd.to_datetime(timestamp) - pd.DateOffset(hours=hr_offset) \n",
    "    data = df.copy()\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
    "    data = data[(data[\"timestamp\"] <= target_ts + pd.DateOffset(minutes=5)) & (data[\"timestamp\"] > target_ts - pd.DateOffset(minutes=thresh))]\n",
    "    if len(data) == 0: \n",
    "        return None\n",
    "    \n",
    "    # print(target_ts, \"\\n\", data[\"timestamp\"])\n",
    "    closest_index = np.argmin(np.abs(target_ts - pd.to_datetime(data[\"timestamp\"])))\n",
    "    # print(\"Closest\", data.iloc[closest_index][\"timestamp\"])\n",
    "    return data.iloc[closest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                             2022-08-02 13:10:00\n",
       "PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE                     18\n",
       "POGAČARJEV TRG-TRŽNICA                                 17\n",
       "KONGRESNI TRG-ŠUBIČEVA ULICA                           19\n",
       "CANKARJEVA UL.-NAMA                                    25\n",
       "                                             ...         \n",
       "DOLENJSKA C. - STRELIŠČE                                9\n",
       "ROŠKA - STRELIŠKA                                       4\n",
       "LEK - VEROVŠKOVA                                        8\n",
       "VOKA - SLOVENČEVA                                       2\n",
       "SUPERNOVA LJUBLJANA - RUDNIK                            1\n",
       "Name: 1, Length: 84, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(train, \"2022-08-02 14:10:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7739, 84)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_28124\\3738134902.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE',\n",
       "       'POGAČARJEV TRG-TRŽNICA', 'KONGRESNI TRG-ŠUBIČEVA ULICA',\n",
       "       'CANKARJEVA UL.-NAMA', 'BREG', 'GRUDNOVO NABREŽJE-KARLOVŠKA C.',\n",
       "       'MIKLOŠIČEV PARK', 'BAVARSKI DVOR', 'TRG OF-KOLODVORSKA UL.',\n",
       "       ...\n",
       "       'DOLENJSKA C. - STRELIŠČE_1hr', 'DOLENJSKA C. - STRELIŠČE_2hr',\n",
       "       'ROŠKA - STRELIŠKA_1hr', 'ROŠKA - STRELIŠKA_2hr',\n",
       "       'LEK - VEROVŠKOVA_1hr', 'LEK - VEROVŠKOVA_2hr', 'VOKA - SLOVENČEVA_1hr',\n",
       "       'VOKA - SLOVENČEVA_2hr', 'SUPERNOVA LJUBLJANA - RUDNIK_1hr',\n",
       "       'SUPERNOVA LJUBLJANA - RUDNIK_2hr'],\n",
       "      dtype='object', length=250)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for station in stations:\n",
    "    train[f\"{station}_1hr\"] = [None for _ in range(train.shape[0])]\n",
    "    train[f\"{station}_2hr\"] = [None for _ in range(train.shape[0])]\n",
    "    \n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE_1hr    None\n",
       "POGAČARJEV TRG-TRŽNICA_1hr                None\n",
       "KONGRESNI TRG-ŠUBIČEVA ULICA_1hr          None\n",
       "CANKARJEVA UL.-NAMA_1hr                   None\n",
       "BREG_1hr                                  None\n",
       "                                          ... \n",
       "DOLENJSKA C. - STRELIŠČE_1hr              None\n",
       "ROŠKA - STRELIŠKA_1hr                     None\n",
       "LEK - VEROVŠKOVA_1hr                      None\n",
       "VOKA - SLOVENČEVA_1hr                     None\n",
       "SUPERNOVA LJUBLJANA - RUDNIK_1hr          None\n",
       "Name: 0, Length: 83, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0,:][[f\"{station}_1hr\" for station in stations]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows\n",
      "Processed 100 rows\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i, timestamp \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train[\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m      4\u001b[0m    row_1hr \u001b[39m=\u001b[39m closest(train, timestamp, \u001b[39m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m    row_2hr \u001b[39m=\u001b[39m closest(train, timestamp, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m    \u001b[39mif\u001b[39;00m row_1hr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m        \u001b[39m# offset_df.append(row_1hr[stations], ignore_index=True)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m        train\u001b[39m.\u001b[39mloc[i, [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mstation\u001b[39m}\u001b[39;00m\u001b[39m_1hr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m station \u001b[39min\u001b[39;00m stations]] \u001b[39m=\u001b[39m row_1hr[stations]\u001b[39m.\u001b[39mvalues\n",
      "Cell \u001b[1;32mIn[123], line 3\u001b[0m, in \u001b[0;36mclosest\u001b[1;34m(df, timestamp, hr_offset, thresh)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosest\u001b[39m(df, timestamp, hr_offset\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, thresh\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m):\n\u001b[0;32m      2\u001b[0m     target_ts \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(timestamp) \u001b[39m-\u001b[39m pd\u001b[39m.\u001b[39mDateOffset(hours\u001b[39m=\u001b[39mhr_offset) \n\u001b[1;32m----> 3\u001b[0m     data \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m      4\u001b[0m     data[\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(data[\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m     data \u001b[39m=\u001b[39m data[(data[\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m target_ts \u001b[39m+\u001b[39m pd\u001b[39m.\u001b[39mDateOffset(minutes\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)) \u001b[39m&\u001b[39m (data[\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m target_ts \u001b[39m-\u001b[39m pd\u001b[39m.\u001b[39mDateOffset(minutes\u001b[39m=\u001b[39mthresh))]\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\generic.py:6368\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6258\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   6259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy\u001b[39m(\u001b[39mself\u001b[39m: NDFrameT, deep: bool_t \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   6260\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   6261\u001b[0m \u001b[39m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6366\u001b[0m \u001b[39m    dtype: object\u001b[39;00m\n\u001b[0;32m   6367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6368\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mcopy(deep\u001b[39m=\u001b[39;49mdeep)\n\u001b[0;32m   6369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcopy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\internals\\managers.py:649\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    647\u001b[0m     new_axes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n\u001b[1;32m--> 649\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mcopy\u001b[39;49m\u001b[39m\"\u001b[39;49m, deep\u001b[39m=\u001b[39;49mdeep)\n\u001b[0;32m    650\u001b[0m new_refs: \u001b[39mlist\u001b[39m[weakref\u001b[39m.\u001b[39mref \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[39mif\u001b[39;00m deep:\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(b, f)(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:549\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    547\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m    548\u001b[0m \u001b[39mif\u001b[39;00m deep:\n\u001b[1;32m--> 549\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m    550\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)(values, placement\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr_locs, ndim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # offset_df = pd.DataFrame(columns=[f\"{station}_1hr\" for station in stations] + [f\"{station}_2hr\" for station in stations])\n",
    "\n",
    "# for i, timestamp in enumerate(train[\"timestamp\"]):\n",
    "#     row_1hr = closest(train, timestamp, 1)\n",
    "#     row_2hr = closest(train, timestamp, 2)\n",
    "    \n",
    "#     if row_1hr is not None:\n",
    "#         # offset_df.append(row_1hr[stations], ignore_index=True)\n",
    "#         train.loc[i, [f\"{station}_1hr\" for station in stations]] = row_1hr[stations].values\n",
    "        \n",
    "#     if row_2hr is not None:\n",
    "#         train.loc[i, [f\"{station}_2hr\" for station in stations]] = row_2hr[stations].values\n",
    "    \n",
    "    \n",
    "#     if i % 100 == 0:\n",
    "#         print(f\"Processed {i} rows\")\n",
    "    \n",
    "# Remove all rows with None\n",
    "# train = train.dropna()\n",
    "#     train[f\"{station}_1hr\"] = train.columns.apply(lambda station: find_closest_smaller_row(train, time, 1, station))\n",
    "#     train[f\"{station}_2hr\"] = train[\"timestamp\"].apply(lambda time: find_closest_smaller_row(train, time, 2, station))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows\n"
     ]
    }
   ],
   "source": [
    "# for i, timestamp in enumerate(test[\"timestamp\"]):\n",
    "#     row_1hr = closest(train, timestamp, 1)\n",
    "#     row_2hr = closest(train, timestamp, 2)\n",
    "    \n",
    "#     if row_1hr is not None:\n",
    "#         # offset_df.append(row_1hr[stations], ignore_index=True)\n",
    "#         test.loc[i, [f\"{station}_1hr\" for station in stations]] = row_1hr[stations].values\n",
    "        \n",
    "#     if row_2hr is not None:\n",
    "#         test.loc[i, [f\"{station}_2hr\" for station in stations]] = row_2hr[stations].values\n",
    "    \n",
    "    \n",
    "#     if i % 100 == 0:\n",
    "#         print(f\"Processed {i} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.to_csv(\"data/enriched_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/test_train.csv\")\n",
    "test = pd.read_csv(\"data/enriched_test.csv\")\n",
    "# train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(\"data/test_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train\n",
    "# d = pd.read_csv(\"data/test_train.csv\")\n",
    "ix = 12\n",
    "for station in stations:\n",
    "    if d.iloc[ix][station] - d.iloc[ix + 12][f'{station}_1hr'] != 0:\n",
    "        print(f\"{station} time: {d.iloc[ix]['timestamp']} - {d.iloc[ix + 12]['timestamp']}: n-bikes vs n_bikes_1hr {d.iloc[ix][station]} {d.iloc[ix + 12][f'{station}_1hr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(timestamp                             2022-08-02 15:00:00\n",
       " PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE                     19\n",
       " POGAČARJEV TRG-TRŽNICA                                  8\n",
       " KONGRESNI TRG-ŠUBIČEVA ULICA                           11\n",
       " CANKARJEVA UL.-NAMA                                    18\n",
       "                                              ...         \n",
       " LEK - VEROVŠKOVA_2hr                                  8.0\n",
       " VOKA - SLOVENČEVA_1hr                                 0.0\n",
       " VOKA - SLOVENČEVA_2hr                                 3.0\n",
       " SUPERNOVA LJUBLJANA - RUDNIK_1hr                      1.0\n",
       " SUPERNOVA LJUBLJANA - RUDNIK_2hr                      1.0\n",
       " Name: 0, Length: 250, dtype: object,\n",
       " 18)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0], train[\"PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE\"][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE',\n",
       "       'POGAČARJEV TRG-TRŽNICA', 'KONGRESNI TRG-ŠUBIČEVA ULICA',\n",
       "       'CANKARJEVA UL.-NAMA', 'BREG', 'GRUDNOVO NABREŽJE-KARLOVŠKA C.',\n",
       "       'MIKLOŠIČEV PARK', 'BAVARSKI DVOR', 'TRG OF-KOLODVORSKA UL.',\n",
       "       ...\n",
       "       'DOLENJSKA C. - STRELIŠČE_1hr', 'DOLENJSKA C. - STRELIŠČE_2hr',\n",
       "       'ROŠKA - STRELIŠKA_1hr', 'ROŠKA - STRELIŠKA_2hr',\n",
       "       'LEK - VEROVŠKOVA_1hr', 'LEK - VEROVŠKOVA_2hr', 'VOKA - SLOVENČEVA_1hr',\n",
       "       'VOKA - SLOVENČEVA_2hr', 'SUPERNOVA LJUBLJANA - RUDNIK_1hr',\n",
       "       'SUPERNOVA LJUBLJANA - RUDNIK_2hr'],\n",
       "      dtype='object', length=250)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new features to train\n",
    "# train['month'], train['day'], train['hour'], train['minute'], train['day_of_week'], train['is_holiday'] = zip(*train['timestamp'].map(get_time_based_features))\n",
    "# test['month'], test['day'], test['hour'], test['minute'], test['day_of_week'], test['is_holiday'] = zip(*test['timestamp'].map(get_time_based_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode days of week and hours\n",
    "days_of_week = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\n",
    "\n",
    "# for i, day in enumerate(days_of_week):\n",
    "#     train[day] = (train[\"day_of_week\"] == i).astype(int) \n",
    "#     test[day] = (test[\"day_of_week\"] == i).astype(int)\n",
    "\n",
    "\n",
    "# hours = [i for i in range(24)]\n",
    "# for i in hours:\n",
    "#     train[f\"hour_{i}\"] = (train[\"hour\"] == i).astype(int)\n",
    "#     test[f\"hour_{i}\"] = (test[\"hour\"] == i).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(\"data/train_processed.csv\", index=False)\n",
    "# test.to_csv(\"data/test_processed.csv\", index=False)\n",
    "\n",
    "train = pd.read_csv(\"data/train_processed.csv\")\n",
    "test = pd.read_csv(\"data/test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_station_dataframes = {}\n",
    "test_station_dataframes = {}\n",
    "\n",
    "for station in stations:\n",
    "            \n",
    "    for n in range(2):\n",
    "        if n == 0:\n",
    "            dataframe = train\n",
    "        else:\n",
    "            dataframe = test\n",
    "        station_df = pd.DataFrame({\n",
    "            \"timestamp\": dataframe[\"timestamp\"].values,\n",
    "            \"n_bikes\": dataframe[station].values,\n",
    "            \"month\": dataframe[\"month\"].values,\n",
    "            \"day\": dataframe[\"day\"].values,\n",
    "            \"hour\": dataframe[\"hour\"].values,\n",
    "            \"minute\": dataframe[\"minute\"].values,\n",
    "            \"day_of_week\": dataframe[\"day_of_week\"].values,\n",
    "            \"is_holiday\": dataframe[\"is_holiday\"].values,\n",
    "            \"n_bikes_1hr\": dataframe[f\"{station}_1hr\"].values,\n",
    "            \"n_bikes_2hr\": dataframe[f\"{station}_2hr\"].values\n",
    "        })\n",
    "        \n",
    "        days_of_week = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\n",
    "\n",
    "        for i, day in enumerate(days_of_week):\n",
    "            station_df[day] = dataframe[day].values\n",
    "\n",
    "\n",
    "        hours = [i for i in range(24)]\n",
    "        for i in hours:\n",
    "            station_df[f\"hour_{i}\"] = dataframe[f\"hour_{i}\"].values   \n",
    "            \n",
    "        if n == 0:    \n",
    "            train_station_dataframes[station] = station_df\n",
    "        else: test_station_dataframes[station] = station_df\n",
    "        \n",
    "        # station = station.replace(\"/\", \"_\")\n",
    "        # if n == 0:\n",
    "        #     station_df.to_csv(f\"data/stations/{station}_train.csv\", index=False)\n",
    "        # else:\n",
    "        #     station_df.to_csv(f\"data/stations/{station}_test.csv\", index=False)    \n",
    "    # station_df.to_csv(f\"data/stations/{station}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_train, X_test, y_train, y_test=None):\n",
    "    # Fit model\n",
    "    lr = linear_model.Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, solver=\"auto\", tol=1e-10) \n",
    "    # lr = linear_model.LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and score\n",
    "    y_pred = lr.predict(X_test)\n",
    "    \n",
    "    if y_test is not None:\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print(f\"MSE: {mse}\")\n",
    "    \n",
    "    return lr, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test=None):\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_test)\n",
    "    return rf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_ridge(X_train, X_test, y_train, y_test=None):\n",
    "    # Fit model\n",
    "    kr = KernelRidge(alpha=1, kernel=\"rbf\") \n",
    "    # lr = linear_model.LinearRegression()\n",
    "    kr.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and score\n",
    "    y_pred = kr.predict(X_test)\n",
    "    \n",
    "    return kr, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr(X_train, X_test, y_train, y_test=None, kernel=\"rbf\"):\n",
    "    # Fit model\n",
    "    svr = SVR() \n",
    "    # lr = linear_model.LinearRegression()\n",
    "    svr.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and score\n",
    "    y_pred = svr.predict(X_test)\n",
    "    \n",
    "    return svr, y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use train set to get good results first and then use test set on classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 7.747587420497177\n"
     ]
    }
   ],
   "source": [
    "avg_mse = 0\n",
    "# Perform linear regression on all stations\n",
    "for station in stations:\n",
    "    station_df = train_station_dataframes[station]\n",
    "    \n",
    "    X = station_df.drop([\"timestamp\", \"n_bikes\", \"minute\"], axis=1)\n",
    "    y = station_df[\"n_bikes\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)   \n",
    "        \n",
    "    X_train_1hr = X_train[X_train.index % 2 == 0]\n",
    "    X_train_2hr = X_train[X_train.index % 2 == 1]\n",
    "    \n",
    "    y_train_1hr = y_train[y_train.index % 2 == 0]\n",
    "    y_train_2hr = y_train[y_train.index % 2 == 1]\n",
    "    \n",
    "    X_test_1hr = X_test[X_test.index % 2 == 0]\n",
    "    X_test_2hr = X_test[X_test.index % 2 == 1]\n",
    "    \n",
    "    # No data for 1hr back\n",
    "    X_train_2hr = np.array(X_train_2hr[\"n_bikes_2hr\"]).reshape(-1, 1) \n",
    "    X_test_2hr = np.array(X_test_2hr[\"n_bikes_2hr\"]).reshape(-1, 1)\n",
    "    \n",
    "    lr_1h, y_pred_1h = linear_regression(X_train_1hr, X_test_1hr, y_train_1hr)\n",
    "    lr_2h, y_pred_2h = linear_regression(X_train_2hr, X_test_2hr, y_train_2hr)\n",
    "    \n",
    "    # rf_1h, y_pred_1h = random_forest(X_train_1hr, X_test_1hr, y_train_1hr)\n",
    "    # rf_2h, y_pred_2h = random_forest(X_train_2hr, X_test_2hr, y_train_2hr)\n",
    "    \n",
    "    y_pred = np.empty(len(y_pred_1h) + len(y_pred_2h))\n",
    "    y_pred[::2] = y_pred_1h\n",
    "    # print(y_pred_1h, y_pred_2h)\n",
    "    y_pred[1::2] = y_pred_2h\n",
    "    \n",
    "    avg_mse += mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    y_pred = pd.DataFrame({\"n_bikes\": y_pred})\n",
    "\n",
    "avg_mse /= len(stations)\n",
    "        \n",
    "print(f\"Score: {avg_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(\"data/test_expanded.csv\")\n",
    "output = pd.read_csv(\"data/bicikelj_test.csv\").copy()\n",
    "\n",
    "# Perform linear regression on all stations\n",
    "for station in stations:\n",
    "    station_df = train_station_dataframes[station]\n",
    "    \n",
    "    X_train = station_df.drop([\"timestamp\", \"n_bikes\"], axis=1)\n",
    "    y_train = station_df[\"n_bikes\"]\n",
    "    \n",
    "    X_test = test_station_dataframes[station].drop([\"timestamp\", \"n_bikes\"], axis=1)\n",
    "    \n",
    "    X_train_1hr = X_train[X_train.index % 2 == 0]\n",
    "    X_train_2hr = X_train[X_train.index % 2 == 1]\n",
    "    y_train_1hr = y_train[y_train.index % 2 == 0]\n",
    "    y_train_2hr = y_train[y_train.index % 2 == 1]\n",
    "    X_test_1hr = X_test[X_test.index % 2 == 0]\n",
    "    X_test_2hr = X_test[X_test.index % 2 == 1]\n",
    "    \n",
    "    \n",
    "    # No data for 1hr back\n",
    "    X_train_2hr = np.array(X_train_2hr[\"n_bikes_2hr\"]).reshape(-1, 1) \n",
    "    X_test_2hr = np.array(X_test_2hr[\"n_bikes_2hr\"]).reshape(-1, 1)\n",
    "    \n",
    "    lr_1h, y_pred_1h = linear_regression(X_train_1hr, X_test_1hr, y_train_1hr)\n",
    "    lr_2h, y_pred_2h = linear_regression(X_train_2hr, X_test_2hr, y_train_2hr)\n",
    "    \n",
    "    # rf_1h, y_pred_1h = random_forest(X_train_1hr, X_test_1hr, y_train_1hr)\n",
    "    # rf_2h, y_pred_2h = random_forest(X_train_2hr, X_test_2hr, y_train_2hr)\n",
    "    \n",
    "    y_pred = np.empty(len(y_pred_1h) + len(y_pred_2h))\n",
    "    y_pred[::2] = y_pred_1h\n",
    "    # print(y_pred_1h, y_pred_2h)\n",
    "    y_pred[1::2] = y_pred_2h\n",
    "    \n",
    "    y_pred = pd.DataFrame({\"n_bikes\": y_pred})\n",
    "    # m = lr_1h.coef_.argsort()[::-1]\n",
    "    \n",
    "    # for i in range(len(m)):\n",
    "        \n",
    "    #     print(X_train.columns[m[i]], lr.coef_[m[i]])\n",
    "    output[station] = y_pred\n",
    "    \n",
    "output.to_csv(\"data/output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
